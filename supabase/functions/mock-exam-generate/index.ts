import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
};

// Direct API Keys
const GEMINI_API_KEY = Deno.env.get("GEMINI_API_KEY");
const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY");
const COHERE_API_KEY = Deno.env.get("COHERE_API_KEY");
const ELEVENLABS_API_KEY = Deno.env.get("ELEVENLABS_API_KEY");

// TTS Voice Presets - Korean Native Voices (High Quality)
// ì—¬ì„±: Seoyeon - í•œêµ­ì–´ ë„¤ì´í‹°ë¸Œ, ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•œ ë°œìŒ
// ë‚¨ì„±: Junwoo - í•œêµ­ì–´ ë„¤ì´í‹°ë¸Œ, ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•œ ë°œìŒ
const KOREAN_VOICES = {
  female: "yoZ06aMxZJJ28mfd3POQ", // Seoyeon - Korean native female
  male: "ODq5zmih8GrVes37Dizd",   // Junwoo - Korean native male
} as const;

const TTS_PRESETS = {
  exam: {
    voiceId: KOREAN_VOICES.female, // Default to female Korean native
    stability: 0.75,
    similarity_boost: 0.85,
    style: 0.15,
    speed: 0.8, // Slower for exam (clear delivery)
  },
  learning: {
    voiceId: KOREAN_VOICES.female,
    stability: 0.7,
    similarity_boost: 0.8,
    style: 0.2,
    speed: 0.75, // Even slower for learners
  },
  natural: {
    voiceId: KOREAN_VOICES.male,
    stability: 0.5,
    similarity_boost: 0.75,
    style: 0.4,
    speed: 0.9,
  },
  formal: {
    voiceId: KOREAN_VOICES.female,
    stability: 0.85,
    similarity_boost: 0.9,
    style: 0.1,
    speed: 0.85,
  },
} as const;

// RAG Configuration
const RAG_CONFIG = {
  MATCH_THRESHOLD: 0.25,
  MATCH_COUNT: 20,
  RERANK_MODEL: 'rerank-v3.5',
  TOP_N: 8,
  EMBEDDING_MODEL: 'text-embedding-3-large',
  EMBEDDING_DIMENSIONS: 1536,
};

interface GenerateRequest {
  examType: "topik1" | "topik2";
  section: "listening" | "reading" | "writing";
  difficulty: "beginner" | "intermediate" | "advanced";
  topic?: string;
  questionCount: number;
  referenceDocUrl?: string;
  referenceDocContent?: string;
  useRag?: boolean;
  generateAudio?: boolean;
  examRound?: number;
  ttsPreset?: keyof typeof TTS_PRESETS;
  stream?: boolean;
  // ë“£ê¸° ì„¸ë¶€ ì„¤ì •
  listeningQuestionType?: string;
  dialogueLength?: string;
  speakerCount?: string;
}

interface GeneratedQuestion {
  question_text: string;
  options: string[];
  correct_answer: number;
  explanation_ko: string;
  explanation_en: string;
  explanation_vi: string;
  part_number: number;
  question_number: number;
  grammar_points: string[];
  vocabulary: string[];
  difficulty: string;
  topic: string;
  listening_script?: string;
  question_audio_url?: string;
  question_image_url?: string;
  image_description?: string; // AIê°€ ìƒì„±í•  ì´ë¯¸ì§€ ì„¤ëª…
}

// Generate embedding using OpenAI
async function generateEmbedding(text: string): Promise<number[]> {
  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: RAG_CONFIG.EMBEDDING_MODEL,
      input: text,
      dimensions: RAG_CONFIG.EMBEDDING_DIMENSIONS,
    }),
  });

  if (!response.ok) {
    throw new Error(`OpenAI embedding error: ${response.status}`);
  }

  const data = await response.json();
  return data.data[0].embedding;
}

// Cohere Rerank
async function rerankResults(
  query: string,
  documents: any[],
  topN: number
): Promise<any[]> {
  if (documents.length === 0 || !COHERE_API_KEY) return documents.slice(0, topN);

  const response = await fetch('https://api.cohere.ai/v1/rerank', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${COHERE_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: RAG_CONFIG.RERANK_MODEL,
      query,
      documents: documents.map(d => d.content),
      top_n: Math.min(topN, documents.length),
      return_documents: false,
    }),
  });

  if (!response.ok) {
    console.error('Cohere rerank error, using fallback');
    return documents.slice(0, topN);
  }

  const data = await response.json();
  return data.results.map((result: any) => ({
    ...documents[result.index],
    rerank_score: result.relevance_score,
  }));
}

// RAG Search - Enhanced for listening scripts
async function ragSearch(
  query: string, 
  supabase: any, 
  section?: string,
  difficulty?: string
): Promise<string> {
  try {
    console.log('ğŸ” RAG search for:', query);
    
    // For listening section, create specialized queries for script patterns
    const queries: string[] = [query];
    
    if (section === 'listening') {
      // Add specialized listening script queries
      queries.push(
        'TOPIK ë“£ê¸° ëŒ€ë³¸ ìŠ¤í¬ë¦½íŠ¸ ëŒ€í™” íŒ¨í„´',
        'TOPIK listening script ë‚¨ì ì—¬ì ëŒ€í™”',
        'ë“£ê¸° ì‹œí—˜ ëŒ€í™”ë¬¸ ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸',
        `TOPIK ë“£ê¸° ${difficulty === 'beginner' ? 'ì´ˆê¸‰' : difficulty === 'advanced' ? 'ê³ ê¸‰' : 'ì¤‘ê¸‰'} ëŒ€í™”`
      );
    }
    
    // Collect all search results from multiple queries
    const allResults: any[] = [];
    const seenIds = new Set<string>();
    
    for (const q of queries) {
      const queryEmbedding = await generateEmbedding(q);
      
      const { data: searchResults, error } = await supabase.rpc(
        'search_knowledge',
        {
          query_embedding: `[${queryEmbedding.join(',')}]`,
          match_threshold: RAG_CONFIG.MATCH_THRESHOLD,
          match_count: section === 'listening' ? 30 : RAG_CONFIG.MATCH_COUNT, // More results for listening
        }
      );

      if (!error && searchResults?.length) {
        for (const result of searchResults) {
          if (!seenIds.has(result.id)) {
            seenIds.add(result.id);
            allResults.push(result);
          }
        }
      }
    }

    if (allResults.length === 0) {
      console.log('No RAG results found');
      return '';
    }

    console.log(`ğŸ“š Found ${allResults.length} total RAG results`);

    // For listening, prioritize script-related content
    let filteredResults = allResults;
    if (section === 'listening') {
      const scriptPatterns = ['ëŒ€ë³¸', 'ìŠ¤í¬ë¦½íŠ¸', 'script', 'ë‚¨ì:', 'ì—¬ì:', 'ëŒ€í™”', 'ë“£ê¸°'];
      const scored = allResults.map(r => {
        let score = r.similarity || 0;
        const content = r.content.toLowerCase();
        
        // Boost score for script-related content
        for (const pattern of scriptPatterns) {
          if (content.includes(pattern.toLowerCase())) {
            score += 0.1;
          }
        }
        
        // Extra boost for actual dialogue patterns
        if (content.includes('ë‚¨ì:') && content.includes('ì—¬ì:')) {
          score += 0.3;
        }
        if (content.match(/[ê°€-í£]+:\s*[ê°€-í£]/)) {
          score += 0.2;
        }
        
        return { ...r, boosted_score: score };
      });
      
      // Sort by boosted score
      scored.sort((a, b) => b.boosted_score - a.boosted_score);
      filteredResults = scored.slice(0, 40); // Take top 40 for reranking
    }

    // Rerank with listening-specific query
    const rerankQuery = section === 'listening' 
      ? `TOPIK ë“£ê¸° ëŒ€ë³¸ ìŠ¤í¬ë¦½íŠ¸ ëŒ€í™” ë‚¨ì ì—¬ì ${query}`
      : query;
    
    const topN = section === 'listening' ? 12 : RAG_CONFIG.TOP_N; // More context for listening
    const rerankedResults = await rerankResults(rerankQuery, filteredResults, topN);
    
    const context = rerankedResults.map((r: any, i: number) => 
      `[ì°¸ê³ ìë£Œ ${i + 1}] (${r.document_title || 'TOPIK ìë£Œ'})${r.boosted_score ? ` [ìŠ¤ì½”ì–´: ${r.boosted_score.toFixed(2)}]` : ''}\n${r.content}`
    ).join('\n\n---\n\n');

    console.log(`âœ… RAG found ${rerankedResults.length} relevant documents (listening enhanced: ${section === 'listening'})`);
    return context;
  } catch (error) {
    console.error('RAG search error:', error);
    return '';
  }
}

// Generate TTS audio using ElevenLabs with preset
// - Removes speaker labels like "ë‚¨ì:"/"ì—¬ì:" from spoken audio
// - If multiple speakers are detected, alternates voices per speaker and concatenates segments
async function generateListeningAudio(
  script: string,
  questionNumber: number,
  examType: string,
  examRound: number,
  supabase: any,
  preset: keyof typeof TTS_PRESETS = "exam",
): Promise<string | null> {
  if (!ELEVENLABS_API_KEY || !script) return null;

  const detectSpeaker = (raw: string): { speakerKey: "male" | "female" | "other"; text: string } => {
    const line = raw.trim();
    // Examples: "ë‚¨ì:", "ì—¬ì:", "ë‚¨:", "ì—¬:", "A:", "B:" (Korean/ASCII colons)
    const m = line.match(/^\s*(ë‚¨ì|ì—¬ì|ë‚¨ì„±|ì—¬ì„±|ë‚¨|ì—¬|A|B|C|D)\s*[:ï¼š]\s*(.*)$/i);
    if (!m) return { speakerKey: "other", text: line };

    const label = String(m[1]).toLowerCase();
    const text = String(m[2] ?? "").trim();

    if (["ì—¬ì", "ì—¬ì„±", "ì—¬"].includes(label)) return { speakerKey: "female", text };
    if (["ë‚¨ì", "ë‚¨ì„±", "ë‚¨"].includes(label)) return { speakerKey: "male", text };

    // A/B/C/D: treat as alternating speakers but default to other
    return { speakerKey: "other", text };
  };

  const stripLeadingId3 = (buf: ArrayBuffer): Uint8Array => {
    const bytes = new Uint8Array(buf);
    // ID3 header: "ID3" + ver(2) + flags(1) + size(4, syncsafe)
    if (bytes.length >= 10 && bytes[0] === 0x49 && bytes[1] === 0x44 && bytes[2] === 0x33) {
      const size =
        ((bytes[6] & 0x7f) << 21) |
        ((bytes[7] & 0x7f) << 14) |
        ((bytes[8] & 0x7f) << 7) |
        (bytes[9] & 0x7f);
      const start = Math.min(bytes.length, 10 + size);
      return bytes.slice(start);
    }
    return bytes;
  };

  const concatBytes = (parts: Uint8Array[]): Uint8Array => {
    const total = parts.reduce((acc, p) => acc + p.length, 0);
    const out = new Uint8Array(total);
    let offset = 0;
    for (const p of parts) {
      out.set(p, offset);
      offset += p.length;
    }
    return out;
  };

  try {
    console.log(`ğŸµ Generating audio for Q${questionNumber} with ${preset} preset...`);

    const baseSettings = TTS_PRESETS[preset];

    // Split into segments (multi-speaker) by line breaks; ignore empty lines
    const rawLines = script
      .split(/\r?\n/)
      .map((l) => l.trim())
      .filter(Boolean);

    const segments = rawLines.length
      ? rawLines.map(detectSpeaker).filter((s) => s.text)
      : [{ speakerKey: "other" as const, text: script.trim() }];

    const uniqueSpeakers = new Set(segments.map((s) => s.speakerKey));
    const isMultiSpeaker = uniqueSpeakers.size >= 2 && (uniqueSpeakers.has("male") || uniqueSpeakers.has("female"));

    const voiceBySpeaker: Record<"male" | "female" | "other", string> = {
      female: KOREAN_VOICES.female, // Seoyeon - Korean native female
      male: KOREAN_VOICES.male,     // Junwoo - Korean native male
      other: baseSettings.voiceId,
    };

    // Build audio buffer(s)
    const audioParts: Uint8Array[] = [];

    for (let i = 0; i < segments.length; i++) {
      const seg = segments[i];
      const voiceId = isMultiSpeaker ? voiceBySpeaker[seg.speakerKey] : baseSettings.voiceId;

      // Add a short pause between lines by punctuation (natural for TTS)
      const text = seg.text.endsWith(".") || seg.text.endsWith("?") || seg.text.endsWith("!")
        ? `${seg.text} ...`
        : `${seg.text}. ...`;

      const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`, {
        method: "POST",
        headers: {
          "xi-api-key": ELEVENLABS_API_KEY,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          text,
          model_id: "eleven_multilingual_v2",
          output_format: "mp3_44100_128",
          voice_settings: {
            stability: baseSettings.stability,
            similarity_boost: baseSettings.similarity_boost,
            style: baseSettings.style,
            speed: baseSettings.speed,
          },
        }),
      });

      if (!response.ok) {
        const t = await response.text().catch(() => "");
        console.error("ElevenLabs TTS error:", response.status, t);
        return null;
      }

      const buf = await response.arrayBuffer();
      // Strip ID3 on subsequent segments to avoid repeated headers when concatenating
      const bytes = i === 0 ? new Uint8Array(buf) : stripLeadingId3(buf);
      audioParts.push(bytes);
    }

    const finalBytes = audioParts.length === 1 ? audioParts[0] : concatBytes(audioParts);

    const fileName = `mock-exam/${examType}/${examRound}/listening_q${questionNumber}_${Date.now()}.mp3`;

    const { error: uploadError } = await supabase.storage
      .from("podcast-audio")
      .upload(fileName, finalBytes.buffer, {
        contentType: "audio/mpeg",
        upsert: true,
      });

    if (uploadError) {
      console.error("Audio upload error:", uploadError);
      return null;
    }

    const { data: urlData } = supabase.storage.from("podcast-audio").getPublicUrl(fileName);

    console.log(`âœ… Audio generated for Q${questionNumber} (multiSpeaker: ${isMultiSpeaker})`);
    return urlData.publicUrl;
  } catch (error) {
    console.error("TTS generation error:", error);
    return null;
  }
}

// Generate image for picture dialogue questions [5-8] using Lovable AI (Gemini Image)
async function generateQuestionImage(
  imageDescription: string,
  questionNumber: number,
  examType: string,
  examRound: number,
  supabase: any,
): Promise<string | null> {
  const LOVABLE_API_KEY = Deno.env.get("LOVABLE_API_KEY");
  if (!LOVABLE_API_KEY || !imageDescription) return null;

  try {
    console.log(`ğŸ–¼ï¸ Generating image for Q${questionNumber}: ${imageDescription.slice(0, 50)}...`);

    // Create a prompt for Korean language test picture dialogue
    const imagePrompt = `Create a simple, clear illustration for a Korean language test (TOPIK). 
The scene: ${imageDescription}
Style requirements:
- Clean, simple line art or illustration style suitable for educational materials
- No text or speech bubbles in the image
- Clear visual elements that match the dialogue context
- Appropriate for all ages
- Similar to official TOPIK test illustrations
- Neutral, professional educational look`;

    const response = await fetch("https://ai.gateway.lovable.dev/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${LOVABLE_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "google/gemini-2.5-flash-image-preview",
        messages: [
          {
            role: "user",
            content: imagePrompt,
          },
        ],
        modalities: ["image", "text"],
      }),
    });

    if (!response.ok) {
      console.error("Lovable AI image generation error:", response.status);
      return null;
    }

    const data = await response.json();
    const imageBase64 = data.choices?.[0]?.message?.images?.[0]?.image_url?.url;

    if (!imageBase64 || !imageBase64.startsWith("data:image/")) {
      console.error("No valid image in response");
      return null;
    }

    // Extract base64 data and convert to Uint8Array
    const base64Data = imageBase64.split(",")[1];
    const binaryString = atob(base64Data);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }

    // Determine file extension from data URL
    const mimeMatch = imageBase64.match(/data:image\/(\w+);/);
    const extension = mimeMatch ? mimeMatch[1] : "png";

    const fileName = `mock-exam/${examType}/${examRound}/picture_q${questionNumber}_${Date.now()}.${extension}`;

    const { error: uploadError } = await supabase.storage
      .from("podcast-audio") // Reuse existing bucket
      .upload(fileName, bytes.buffer, {
        contentType: `image/${extension}`,
        upsert: true,
      });

    if (uploadError) {
      console.error("Image upload error:", uploadError);
      return null;
    }

    const { data: urlData } = supabase.storage.from("podcast-audio").getPublicUrl(fileName);

    console.log(`âœ… Image generated for Q${questionNumber}`);
    return urlData.publicUrl;
  } catch (error) {
    console.error("Image generation error:", error);
    return null;
  }
}

// Build system prompt for Gemini
function buildSystemPrompt(params: GenerateRequest, ragContext: string): string {
  const levelInfo = {
    topik1: "TOPIK I (1-2ê¸‰, ì´ˆê¸‰-ì¤‘ê¸‰ ìˆ˜ì¤€)",
    topik2: "TOPIK II (3-6ê¸‰, ì¤‘ê¸‰-ê³ ê¸‰ ìˆ˜ì¤€)",
  };

  const sectionInfo = {
    listening: "ë“£ê¸° (Listening)",
    reading: "ì½ê¸° (Reading)",
    writing: "ì“°ê¸° (Writing)",
  };

  const difficultyInfo = {
    beginner: "ì´ˆê¸‰ (1-2ê¸‰ ìˆ˜ì¤€, ê¸°ë³¸ ì–´íœ˜ì™€ ê°„ë‹¨í•œ ë¬¸ì¥ êµ¬ì¡°)",
    intermediate: "ì¤‘ê¸‰ (3-4ê¸‰ ìˆ˜ì¤€, ë‹¤ì–‘í•œ ì£¼ì œì™€ ë³µì¡í•œ ë¬¸ì¥)",
    advanced: "ê³ ê¸‰ (5-6ê¸‰ ìˆ˜ì¤€, ì „ë¬¸ì  ë‚´ìš©ê³¼ ì¶”ìƒì  ê°œë…)",
  };

  let prompt = `ë‹¹ì‹ ì€ TOPIK(í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜) ì „ë¬¸ ì¶œì œìœ„ì›ì…ë‹ˆë‹¤. 
ìµœê³  í’ˆì§ˆì˜ TOPIK ëª¨ì˜ê³ ì‚¬ ë¬¸ì œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

## ì¶œì œ ì¡°ê±´
- ì‹œí—˜ ìœ í˜•: ${levelInfo[params.examType]}
- ì˜ì—­: ${sectionInfo[params.section]}
- ë‚œì´ë„: ${difficultyInfo[params.difficulty]}
- ìƒì„±í•  ë¬¸ì œ ìˆ˜: ${params.questionCount}ê°œ
${params.topic ? `- ì£¼ì œ/ë¬¸ë²•: ${params.topic}` : ''}

## ì¶œì œ ì›ì¹™
1. ì‹¤ì œ TOPIK ì‹œí—˜ í˜•ì‹ê³¼ 100% ë™ì¼í•œ ë¬¸ì œ êµ¬ì¡°
2. ì •í™•í•œ í•œêµ­ì–´ ë¬¸ë²•ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
3. ëª…í™•í•˜ê³  êµìœ¡ì ì¸ í•´ì„¤ (í•œêµ­ì–´, ì˜ì–´, ë² íŠ¸ë‚¨ì–´)
4. ê° ë³´ê¸°ëŠ” í•©ë¦¬ì ì´ê³  ë‚œì´ë„ì— ë§ëŠ” ì˜¤ë‹µ ì„ ì§€
5. ë¬¸ë²• í¬ì¸íŠ¸ì™€ í•µì‹¬ ì–´íœ˜ ëª…ì‹œ

## ë¬¸ì œ ìœ í˜• ê°€ì´ë“œ`;

  if (params.section === 'reading') {
    prompt += `
### ì½ê¸° ì˜ì—­ ë¬¸ì œ ìœ í˜•
- [1~4] ë¹ˆì¹¸ ì™„ì„± (ì–´íœ˜/ë¬¸ë²•)
- [5~9] ì£¼ì œ/ì œëª© ì°¾ê¸°
- [10~13] ê¸€ì˜ ë‚´ìš©ê³¼ ê°™ì€ ê²ƒ ê³ ë¥´ê¸°
- [14~20] ë¹ˆì¹¸ ì™„ì„± (ë¬¸ë§¥)
- [21~30] ì§€ë¬¸ ë…í•´ í›„ ì§ˆë¬¸ ì‘ë‹µ

ê° ë¬¸ì œì—ëŠ” ë°˜ë“œì‹œ ì§€ë¬¸(ì½ê¸° í…ìŠ¤íŠ¸)ì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.`;
  } else if (params.section === 'listening') {
    // ë“£ê¸° ì„¸ë¶€ ì„¤ì • íŒŒì‹±
    const questionType = params.listeningQuestionType || 'mixed';
    const dialogueLen = params.dialogueLength || 'auto';
    const speakers = params.speakerCount || 'auto';

    // ë¬¸ì œ ìœ í˜•ë³„ ì„¤ì •
    const questionTypeGuide: Record<string, { partRange: string; turns: string; description: string }> = {
      "1-4": { partRange: "1~4", turns: "1-2í„´", description: "ì ì ˆí•œ ëŒ€ë‹µ ê³ ë¥´ê¸° (ê°„ë‹¨í•œ ì§ˆë¬¸-ì‘ë‹µ)" },
      "5-8": { partRange: "5~8", turns: "2-3í„´", description: "ê·¸ë¦¼ ë³´ê³  ì•Œë§ì€ ëŒ€í™” ê³ ë¥´ê¸°" },
      "9-12": { partRange: "9~12", turns: "3-4í„´", description: "ëŒ€í™”ì˜ ì¥ì†Œ/í™”ì œ/ëª©ì  íŒŒì•…" },
      "13-16": { partRange: "13~16", turns: "4-6í„´", description: "ì„¸ë¶€ ë‚´ìš© íŒŒì•…" },
      "17-20": { partRange: "17~20", turns: "5-8í„´", description: "í™”ìì˜ ì˜ë„/íƒœë„/í›„ì† í–‰ë™ íŒŒì•…" },
      "21-30": { partRange: "21~30", turns: "6-10í„´", description: "ê¸´ ëŒ€í™”/ë‹´í™” ì¢…í•© ì´í•´" },
    };

    // ëŒ€í™” ê¸¸ì´ ì„¤ì •
    const dialogueLengthGuide: Record<string, string> = {
      short: "ì§§ì€ ëŒ€í™” (1-3í„´)",
      medium: "ì¤‘ê°„ ëŒ€í™” (4-6í„´)",
      long: "ê¸´ ëŒ€í™” (7-10í„´)",
      auto: "ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” ê¸¸ì´",
    };

    // í™”ì ìˆ˜ ì„¤ì •
    const speakerGuide: Record<string, string> = {
      "2": "ë‚¨ì-ì—¬ì 2ì¸ ëŒ€í™”",
      "3": "3ì¸ ëŒ€í™” (ë‚¨1-ì—¬1-ë‚¨2 ë˜ëŠ” ë‚¨1-ì—¬2)",
      "monologue": "1ì¸ ë‹´í™” (ê°•ì˜, ë‰´ìŠ¤, ì•ˆë‚´ë°©ì†¡ ë“±)",
      "auto": "ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” í™”ì êµ¬ì„±",
    };

    prompt += `
### ë“£ê¸° ì˜ì—­ ë¬¸ì œ ìœ í˜•
${questionType === 'mixed' ? `
- [1~4] ì ì ˆí•œ ëŒ€ë‹µ ê³ ë¥´ê¸° (ê°„ë‹¨í•œ ì§ˆë¬¸-ì‘ë‹µ)
- [5~8] ê·¸ë¦¼ ë³´ê³  ì•Œë§ì€ ëŒ€í™” ê³ ë¥´ê¸°
- [9~12] ëŒ€í™”ì˜ ì¥ì†Œ/í™”ì œ/ëª©ì  íŒŒì•…
- [13~16] ì„¸ë¶€ ë‚´ìš© íŒŒì•… (ëŒ€í™” ë‚´ìš©ê³¼ ê°™ì€ ê²ƒ)
- [17~20] í™”ìì˜ ì˜ë„/íƒœë„/í›„ì† í–‰ë™ íŒŒì•…
- [21~30] ê¸´ ëŒ€í™”/ë‹´í™” ë“£ê³  ì¢…í•©ì  ì´í•´

ë‹¤ì–‘í•œ ìœ í˜•ì„ ê³¨ê³ ë£¨ ìƒì„±í•˜ì„¸ìš”.` : `
âš ï¸ **ì§€ì •ëœ ë¬¸ì œ ìœ í˜•**: [${questionTypeGuide[questionType]?.partRange}ë²ˆ ìœ í˜•]
- ìœ í˜•: ${questionTypeGuide[questionType]?.description}
- ê¶Œì¥ ëŒ€í™” ê¸¸ì´: ${questionTypeGuide[questionType]?.turns}

ëª¨ë“  ë¬¸ì œë¥¼ ì´ ìœ í˜•ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.`}

### ğŸ§ ëŒ€í™” ì„¤ì •
- **ëŒ€í™” ê¸¸ì´**: ${dialogueLengthGuide[dialogueLen] || dialogueLengthGuide.auto}
- **í™”ì êµ¬ì„±**: ${speakerGuide[speakers] || speakerGuide.auto}

### ğŸµ ë“£ê¸° ìŠ¤í¬ë¦½íŠ¸ (listening_script) - ë§¤ìš° ì¤‘ìš”!

**ë°˜ë“œì‹œ ì°¸ê³ ìë£Œ(RAG)ì— ìˆëŠ” ì‹¤ì œ TOPIK ë“£ê¸° ëŒ€ë³¸ íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.**

ë“£ê¸° ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì›ì¹™:
1. **í™”ì í‘œì‹œ**: ë°˜ë“œì‹œ "ë‚¨ì:" / "ì—¬ì:" ë˜ëŠ” "ë‚¨:" / "ì—¬:" í˜•ì‹ ì‚¬ìš©${speakers === '3' ? ' (3ì¸: ë‚¨1, ì—¬1, ë‚¨2 ë“±)' : ''}${speakers === 'monologue' ? ' (1ì¸ ë‹´í™”: "í™”ì:" ë˜ëŠ” ë‚´ìš©ë§Œ)' : ''}
2. **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ì‹¤ì œ í•œêµ­ì–´ ëŒ€í™”ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ (ì¶•ì•½, ì¡°ì‚¬ ìƒëµ ë“±)
3. **ëŒ€í™” ê¸¸ì´**: ${dialogueLen === 'short' ? '1-3í„´ì˜ ì§§ì€ ëŒ€í™”' : dialogueLen === 'medium' ? '4-6í„´ì˜ ì¤‘ê°„ ëŒ€í™”' : dialogueLen === 'long' ? '7-10í„´ì˜ ê¸´ ëŒ€í™”' : 'ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” ì ì ˆí•œ ê¸¸ì´'}
4. **ë§¥ë½ ëª…í™•ì„±**: ìŠ¤í¬ë¦½íŠ¸ë§Œ ë³´ê³ ë„ ì •ë‹µì„ ë…¼ë¦¬ì ìœ¼ë¡œ ë„ì¶œí•  ìˆ˜ ìˆì–´ì•¼ í•¨
5. **ì˜¤ë‹µ ì„ ì§€ íƒ€ë‹¹ì„±**: ì˜¤ë‹µë„ ê·¸ëŸ´ë“¯í•´ì•¼ í•˜ì§€ë§Œ, ìŠ¤í¬ë¦½íŠ¸ì— ëª…í™•í•œ ê·¼ê±°ê°€ ì—†ì–´ì•¼ í•¨

ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ (ìœ í˜•ë³„):

[1~4ë²ˆ ìœ í˜• - ì ì ˆí•œ ëŒ€ë‹µ]
"ì—¬ì: ì˜¤ëŠ˜ ì €ë…ì— ë­ í•  ê±°ì˜ˆìš”?"

[5~8ë²ˆ ìœ í˜• - ê·¸ë¦¼ ëŒ€í™”]
"ë‚¨ì: ì´ ì±… ì–´ë””ì— ë†“ì„ê¹Œìš”?\\nì—¬ì: ì € ì±…ìƒ ìœ„ì— ë†“ì•„ ì£¼ì„¸ìš”."

[9~12ë²ˆ ìœ í˜• - ì¥ì†Œ/í™”ì œ]
"ì—¬ì: ì–´ì„œ ì˜¤ì„¸ìš”. ë­˜ ì°¾ìœ¼ì„¸ìš”?\\në‚¨ì: ê°ê¸°ì•½ ì¢€ ì£¼ì„¸ìš”.\\nì—¬ì: ì–´ë–¤ ì¦ìƒì´ ìˆìœ¼ì„¸ìš”?\\në‚¨ì: ê¸°ì¹¨ì´ ë§ì´ ë‚˜ê³  ì—´ë„ ì¢€ ìˆì–´ìš”."

[13~16ë²ˆ ìœ í˜• - ì„¸ë¶€ ë‚´ìš©]
"ë‚¨ì: ì´ë²ˆ ì£¼ë§ì— ì‚°ì— ê°ˆ ê±´ë°, ê°™ì´ ê°ˆë˜?\\nì—¬ì: ì¢‹ì•„. ê·¸ëŸ°ë° ë‚ ì”¨ê°€ ê´œì°®ì„ê¹Œ?\\në‚¨ì: ì¼ê¸°ì˜ˆë³´ ë´¤ëŠ”ë° ë§‘ëŒ€. ì•„ì¹¨ 8ì‹œì— ì¶œë°œí•˜ì.\\nì—¬ì: ì•Œì•˜ì–´. ë„ì‹œë½ì€ ë‚´ê°€ ì¤€ë¹„í• ê²Œ."

[21~30ë²ˆ ìœ í˜• - ë‹´í™”]
"ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ê°•ì˜ì—ì„œëŠ” í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. í•œêµ­ ìŒì‹ì€ ë°œíš¨ ì‹í’ˆì´ ë§ì€ ê²ƒì´ íŠ¹ì§•ì…ë‹ˆë‹¤..."

question_textì—ëŠ” ì§ˆë¬¸ë§Œ ë„£ìœ¼ì„¸ìš”.
- ì¢‹ì€ ì˜ˆ: "ë‚¨ìëŠ” ì™œ ê°ê¸°ì•½ì„ ì‚¬ëŸ¬ ì™”ìŠµë‹ˆê¹Œ?"
- ë‚˜ìœ ì˜ˆ: "(ëŒ€í™”ë¥¼ ë“£ê³ ) ë‚¨ìëŠ”..." (ìŠ¤í¬ë¦½íŠ¸ëŠ” listening_scriptì—)`;
  }

  if (ragContext) {
    prompt += `\n\n## ğŸ“š ì°¸ê³  ìë£Œ (RAG ê²€ìƒ‰ ê²°ê³¼)
ë‹¤ìŒ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì œë¥¼ ì¶œì œí•˜ì„¸ìš”. ì´ ìë£ŒëŠ” ì‹¤ì œ TOPIK ê¸°ì¶œë¬¸ì œ, êµì¬, ì–´íœ˜ ëª©ë¡ ë“±ì…ë‹ˆë‹¤:

${ragContext}

ìœ„ ì°¸ê³  ìë£Œì˜ ì–´íœ˜, ë¬¸ë²•, ë¬¸ì¥ íŒ¨í„´ì„ í™œìš©í•˜ì—¬ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.`;

    if (params.section === 'listening') {
      prompt += `

### âš ï¸ ë“£ê¸° ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì‹œ ì¤‘ìš” ì§€ì¹¨
1. **ìœ„ ì°¸ê³ ìë£Œì—ì„œ ì‹¤ì œ TOPIK ë“£ê¸° ëŒ€ë³¸ íŒ¨í„´ì„ ë¶„ì„í•˜ì„¸ìš”**
2. ëŒ€í™”ì˜ íë¦„, í™”ì êµëŒ€ íŒ¨í„´, í‘œí˜„ ë°©ì‹ì„ ì°¸ê³ í•˜ì„¸ìš”
3. ì°¸ê³ ìë£Œì— ìˆëŠ” ëŒ€í™” êµ¬ì¡°ë¥¼ ëª¨ë°©í•˜ë˜, ìƒˆë¡œìš´ ìƒí™©ìœ¼ë¡œ ë³€í˜•í•˜ì„¸ìš”
4. ì •ë‹µì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ëª…í™•íˆ ë„ì¶œë˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”
5. ì˜¤ë‹µ ì„ ì§€ëŠ” ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ ìŠ¤í¬ë¦½íŠ¸ì™€ ë§ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤`;
    }
  }

  if (params.referenceDocContent) {
    prompt += `\n\n## ğŸ“„ ì—…ë¡œë“œëœ ë ˆí¼ëŸ°ìŠ¤ ë¬¸ì„œ
ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±/ë³€í˜•í•˜ì„¸ìš”:

${params.referenceDocContent}

ì´ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ:
1. ë¬¸ì œ í˜•ì‹ê³¼ ìŠ¤íƒ€ì¼ì„ ìœ ì§€
2. ì§€ì •ëœ ë‚œì´ë„(${params.difficulty})ì— ë§ê²Œ ë³€í˜•
3. ìƒˆë¡œìš´ ìƒí™©/ë§¥ë½ìœ¼ë¡œ ì‘ìš©
4. ìƒì„¸í•œ í•´ì„¤ ì¶”ê°€`;
  }

  prompt += `

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
{
  "questions": [
    {
      "question_text": "ë¬¸ì œ í…ìŠ¤íŠ¸ (ì½ê¸°: ì§€ë¬¸+ì§ˆë¬¸, ë“£ê¸°: ì§ˆë¬¸ë§Œ)",
      "options": ["â‘  ì„ ì§€1", "â‘¡ ì„ ì§€2", "â‘¢ ì„ ì§€3", "â‘£ ì„ ì§€4"],
      "correct_answer": 1-4 ì¤‘ ì •ë‹µ ë²ˆí˜¸,
      "explanation_ko": "ìƒì„¸í•œ í•œêµ­ì–´ í•´ì„¤",
      "explanation_en": "Detailed English explanation",
      "explanation_vi": "Giáº£i thÃ­ch chi tiáº¿t báº±ng tiáº¿ng Viá»‡t",
      "part_number": ë¬¸ì œ íŒŒíŠ¸ ë²ˆí˜¸,
      "question_number": ë¬¸ì œ ë²ˆí˜¸,
      "grammar_points": ["ë¬¸ë²• í¬ì¸íŠ¸1", "ë¬¸ë²• í¬ì¸íŠ¸2"],
      "vocabulary": ["ì–´íœ˜1 (ëœ»)", "ì–´íœ˜2 (ëœ»)"],
      "difficulty": "${params.difficulty}",
      "topic": "${params.topic || 'ì¼ë°˜'}"${params.section === 'listening' ? `,
      "listening_script": "ë‚¨ì: ...\\nì—¬ì: ...",
      "image_description": "[5-8ë²ˆ ê·¸ë¦¼ ë¬¸ì œì¼ ê²½ìš°ë§Œ] ê·¸ë¦¼ì— ë‚˜íƒ€ë‚˜ì•¼ í•  ì¥ë©´/ìƒí™© ì„¤ëª… (ì˜ˆ: 'ì±…ìƒ ìœ„ì— ì±…ì„ ë†“ëŠ” ë‚¨ìì™€ ê·¸ê²ƒì„ ê°€ë¦¬í‚¤ëŠ” ì—¬ì')"` : ''}
    }
  ]
}

âš ï¸ [5-8ë²ˆ ê·¸ë¦¼ ë¬¸ì œ] í•„ìˆ˜ ì§€ì¹¨:
- image_description í•„ë“œì— ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë ¤ì§ˆ ì¥ë©´ì„ ìì„¸íˆ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- ëŒ€í™” ë‚´ìš©ì´ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆëŠ” ì¥ë©´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì˜ˆ: "ì¹´í˜ì—ì„œ ì»¤í”¼ë¥¼ ì£¼ë¬¸í•˜ëŠ” ì†ë‹˜ê³¼ ë°”ë¦¬ìŠ¤íƒ€", "ë„ì„œê´€ì—ì„œ ì±…ì„ ì°¾ëŠ” í•™ìƒ"

ëª¨ë“  í•„ë“œë¥¼ ë°˜ë“œì‹œ ì±„ìš°ì„¸ìš”. ë¹ˆ ê°’ì´ ìˆìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤.`;

  return prompt;
}

// Streaming handler
async function handleStreamingGeneration(
  params: GenerateRequest,
  ragContext: string,
  supabase: any
): Promise<Response> {
  const encoder = new TextEncoder();
  const geminiModel = Deno.env.get("GEMINI_MODEL") || "gemini-2.5-pro";
  const systemPrompt = buildSystemPrompt(params, ragContext);

  const stream = new ReadableStream({
    async start(controller) {
      try {
        // Send progress update
        const sendProgress = (step: string, progress: number, message: string) => {
          const data = JSON.stringify({ type: "progress", step, progress, message });
          controller.enqueue(encoder.encode(`data: ${data}\n\n`));
        };

        sendProgress("rag", 20, "ğŸ“š RAG ê²€ìƒ‰ ì™„ë£Œ");
        
        sendProgress("generating", 30, "ğŸ¤– Gemini 2.5 Pro ë¬¸ì œ ìƒì„± ì‹œì‘...");

        // Call Gemini with streaming
        const geminiResponse = await fetch(
          `https://generativelanguage.googleapis.com/v1beta/models/${geminiModel}:streamGenerateContent?key=${GEMINI_API_KEY}&alt=sse`,
          {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              contents: [{
                role: "user",
                parts: [{
                  text: `${systemPrompt}\n\n---\n\n${params.questionCount}ê°œì˜ ${params.section} ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
${params.topic ? `ì£¼ì œ/ë¬¸ë²•: ${params.topic}` : ''}
ë‚œì´ë„: ${params.difficulty}
ëª¨ë“  ë¬¸ì œëŠ” ì‹¤ì œ TOPIK ì‹œí—˜ê³¼ ë™ì¼í•œ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.`
                }]
              }],
              generationConfig: {
                temperature: 0.7,
                topP: 0.95,
                topK: 40,
                maxOutputTokens: 65536,
                responseMimeType: "application/json",
              },
              safetySettings: [
                { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
                { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
                { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE" },
                { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE" },
              ],
            }),
          }
        );

        if (!geminiResponse.ok) {
          const errorText = await geminiResponse.text();
          throw new Error(`Gemini API error: ${geminiResponse.status} - ${errorText}`);
        }

        // Stream the response
        const reader = geminiResponse.body?.getReader();
        if (!reader) throw new Error("No response body");

        let fullContent = "";
        let chunkCount = 0;
        const decoder = new TextDecoder();

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split('\n');

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              try {
                const jsonStr = line.slice(6);
                if (jsonStr.trim() === '[DONE]') continue;
                
                const parsed = JSON.parse(jsonStr);
                const text = parsed.candidates?.[0]?.content?.parts?.[0]?.text || '';
                
                if (text) {
                  fullContent += text;
                  chunkCount++;
                  
                  // Send token chunk to client
                  const tokenData = JSON.stringify({ type: "token", content: text, count: chunkCount });
                  controller.enqueue(encoder.encode(`data: ${tokenData}\n\n`));
                  
                  // Update progress periodically
                  if (chunkCount % 20 === 0) {
                    const progress = Math.min(30 + (chunkCount / 5), 80);
                    sendProgress("generating", progress, `ğŸ¤– ìƒì„± ì¤‘... (${chunkCount} í† í°)`);
                  }
                }
              } catch (e) {
                // Ignore parse errors for partial chunks
              }
            }
          }
        }

        sendProgress("parsing", 85, "ğŸ“ ìƒì„±ëœ ë¬¸ì œ íŒŒì‹± ì¤‘...");

        // Parse the complete content
        let parsed: { questions: GeneratedQuestion[] };
        try {
          let jsonContent = fullContent;
          if (jsonContent.startsWith("```json")) jsonContent = jsonContent.slice(7);
          if (jsonContent.startsWith("```")) jsonContent = jsonContent.slice(3);
          if (jsonContent.endsWith("```")) jsonContent = jsonContent.slice(0, -3);
          parsed = JSON.parse(jsonContent.trim());
        } catch (e) {
          console.error("Failed to parse Gemini response:", fullContent.slice(0, 500));
          throw new Error("Failed to parse Gemini response as JSON");
        }

        // Validate questions
        let validQuestions = (parsed.questions || []).filter((q) => {
          return (
            q.question_text &&
            Array.isArray(q.options) &&
            q.options.length >= 4 &&
            typeof q.correct_answer === "number" &&
            q.correct_answer >= 1 &&
            q.correct_answer <= 4 &&
            q.explanation_ko
          );
        });

        sendProgress("audio", 88, `âœ… ${validQuestions.length}ê°œ ë¬¸ì œ ìƒì„± ì™„ë£Œ`);

        // Generate images for picture dialogue questions [5-8]
        if (params.section === 'listening' && params.listeningQuestionType === '5-8' && params.examRound) {
          sendProgress("image", 89, "ğŸ–¼ï¸ ê·¸ë¦¼ ë¬¸ì œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...");
          
          for (let i = 0; i < validQuestions.length; i++) {
            const q = validQuestions[i];
            if (q.image_description) {
              sendProgress("image", 89 + (i / validQuestions.length) * 3, `ğŸ–¼ï¸ Q${i + 1} ì´ë¯¸ì§€ ìƒì„± ì¤‘...`);
              
              const imageUrl = await generateQuestionImage(
                q.image_description,
                q.question_number || i + 1,
                params.examType,
                params.examRound,
                supabase
              );
              if (imageUrl) {
                validQuestions[i].question_image_url = imageUrl;
              }
            }
          }
        }

        // Generate audio for listening questions
        if (params.section === 'listening' && params.generateAudio !== false && ELEVENLABS_API_KEY && params.examRound) {
          sendProgress("audio", 92, "ğŸµ TTS ìŒì„± ìƒì„± ì¤‘...");
          
          const ttsPreset = params.ttsPreset || 'exam';
          
          for (let i = 0; i < validQuestions.length; i++) {
            const q = validQuestions[i];
            if (q.listening_script) {
              sendProgress("audio", 92 + (i / validQuestions.length) * 6, `ğŸµ Q${i + 1} ìŒì„± ìƒì„± ì¤‘...`);
              
              const audioUrl = await generateListeningAudio(
                q.listening_script,
                q.question_number || i + 1,
                params.examType,
                params.examRound,
                supabase,
                ttsPreset
              );
              if (audioUrl) {
                validQuestions[i].question_audio_url = audioUrl;
              }
            }
          }
        }

        sendProgress("complete", 100, "ğŸ‰ ìƒì„± ì™„ë£Œ!");

        // Send final result
        const finalData = JSON.stringify({
          type: "complete",
          success: true,
          questions: validQuestions,
          ragUsed: !!ragContext,
          ragDocCount: ragContext ? ragContext.split('---').length : 0,
          model: geminiModel,
        });
        controller.enqueue(encoder.encode(`data: ${finalData}\n\n`));
        controller.close();

      } catch (error: any) {
        console.error("Streaming error:", error);
        const errorData = JSON.stringify({ type: "error", error: error.message });
        controller.enqueue(encoder.encode(`data: ${errorData}\n\n`));
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      ...corsHeaders,
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
    },
  });
}

serve(async (req) => {
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const params: GenerateRequest = await req.json();
    
    console.log("ğŸ¯ Mock Exam Generation Request:", {
      examType: params.examType,
      section: params.section,
      difficulty: params.difficulty,
      topic: params.topic,
      questionCount: params.questionCount,
      useRag: params.useRag,
      generateAudio: params.generateAudio,
      ttsPreset: params.ttsPreset,
      stream: params.stream,
      hasReference: !!params.referenceDocContent,
      // ë“£ê¸° ì„¸ë¶€ ì„¤ì •
      listeningQuestionType: params.listeningQuestionType,
      dialogueLength: params.dialogueLength,
      speakerCount: params.speakerCount,
    });

    if (!GEMINI_API_KEY) {
      throw new Error("GEMINI_API_KEY not configured");
    }

    const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // RAG Search for context - Enhanced for listening section
    let ragContext = '';
    if (params.useRag !== false && OPENAI_API_KEY) {
      const searchQuery = `TOPIK ${params.examType === 'topik1' ? 'I' : 'II'} ${params.section} ${params.difficulty} ${params.topic || ''}`.trim();
      ragContext = await ragSearch(searchQuery, supabase, params.section, params.difficulty);
    }

    // Handle streaming mode
    if (params.stream) {
      return handleStreamingGeneration(params, ragContext, supabase);
    }

    // Non-streaming mode (legacy)
    const systemPrompt = buildSystemPrompt(params, ragContext);
    const geminiModel = Deno.env.get("GEMINI_MODEL") || "gemini-2.5-pro";

    console.log("ğŸ¤– Calling Gemini 2.5 Pro directly...");
    
    const geminiResponse = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/${geminiModel}:generateContent?key=${GEMINI_API_KEY}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [{
            role: "user",
            parts: [{
              text: `${systemPrompt}\n\n---\n\n${params.questionCount}ê°œì˜ ${params.section} ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
${params.topic ? `ì£¼ì œ/ë¬¸ë²•: ${params.topic}` : ''}
ë‚œì´ë„: ${params.difficulty}
ëª¨ë“  ë¬¸ì œëŠ” ì‹¤ì œ TOPIK ì‹œí—˜ê³¼ ë™ì¼í•œ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.`
            }]
          }],
          generationConfig: {
            temperature: 0.7,
            topP: 0.95,
            topK: 40,
            maxOutputTokens: 65536,
            responseMimeType: "application/json",
          },
          safetySettings: [
            { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
            { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
            { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE" },
            { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE" },
          ],
        }),
      }
    );

    if (!geminiResponse.ok) {
      const errorText = await geminiResponse.text();
      console.error("Gemini API error:", geminiResponse.status, errorText);
      throw new Error(`Gemini API error: ${geminiResponse.status} - ${errorText}`);
    }

    const geminiData = await geminiResponse.json();
    const content = geminiData.candidates?.[0]?.content?.parts?.[0]?.text;
    
    if (!content) {
      throw new Error("No content in Gemini response");
    }

    let parsed: { questions: GeneratedQuestion[] };
    try {
      let jsonContent = content;
      if (jsonContent.startsWith("```json")) jsonContent = jsonContent.slice(7);
      if (jsonContent.startsWith("```")) jsonContent = jsonContent.slice(3);
      if (jsonContent.endsWith("```")) jsonContent = jsonContent.slice(0, -3);
      parsed = JSON.parse(jsonContent.trim());
    } catch (e) {
      console.error("Failed to parse Gemini response:", content);
      throw new Error("Failed to parse Gemini response as JSON");
    }

    let validQuestions = (parsed.questions || []).filter((q) => {
      return (
        q.question_text &&
        Array.isArray(q.options) &&
        q.options.length >= 4 &&
        typeof q.correct_answer === "number" &&
        q.correct_answer >= 1 &&
        q.correct_answer <= 4 &&
        q.explanation_ko
      );
    });

    console.log(`âœ… Generated ${validQuestions.length} valid questions`);

    // Generate audio for listening questions
    if (params.section === 'listening' && params.generateAudio !== false && ELEVENLABS_API_KEY && params.examRound) {
      const ttsPreset = params.ttsPreset || 'exam';
      
      for (let i = 0; i < validQuestions.length; i++) {
        const q = validQuestions[i];
        if (q.listening_script) {
          const audioUrl = await generateListeningAudio(
            q.listening_script,
            q.question_number || i + 1,
            params.examType,
            params.examRound,
            supabase,
            ttsPreset
          );
          if (audioUrl) {
            validQuestions[i].question_audio_url = audioUrl;
          }
        }
      }
    }

    return new Response(
      JSON.stringify({
        success: true,
        questions: validQuestions,
        ragUsed: !!ragContext,
        ragDocCount: ragContext ? ragContext.split('---').length : 0,
        model: geminiModel,
      }),
      { headers: { ...corsHeaders, "Content-Type": "application/json" } }
    );

  } catch (error: unknown) {
    console.error("Generation error:", error);
    const errorMessage = error instanceof Error ? error.message : "Unknown error";
    return new Response(
      JSON.stringify({ error: errorMessage }),
      { status: 500, headers: { ...corsHeaders, "Content-Type": "application/json" } }
    );
  }
});
